# TranscribeMCP - GPU-Accelerated Audio Transcription

High-performance audio transcription service with GPU acceleration, speaker diarization, and enterprise-grade reliability.

## üöÄ Key Features

- **GPU Acceleration**: 7x faster processing with NVIDIA CUDA
- **Speaker Diarization**: Automatic speaker identification and labeling
- **Multiple Output Formats**: JSON, TXT, SRT, VTT, TSV
- **Large File Support**: Process hours of audio efficiently
- **Automatic Optimization**: Intelligent GPU/CPU detection and configuration
- **Production Ready**: Enterprise-grade reliability and performance monitoring

## ‚ö° Performance

| Mode | Processing Speed | Best Use Case |
|------|-----------------|---------------|
| **GPU (CUDA)** | **6-7x realtime** | Production workloads, large files |
| **CPU** | 1x realtime | Development, small files |

**Real Example**: 47-minute audio file processed in 6.8 minutes (6.97x realtime) on RTX 3060.

## üõ†Ô∏è Quick Start

### Installation

```bash
# Clone repository
git clone <repository-url>
cd TranscribeMCP

# Create virtual environment
python -m venv transcribe_mcp_env
source transcribe_mcp_env/bin/activate  # Linux/Mac
# transcribe_mcp_env\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Install GPU support (recommended)
pip install torch==2.2.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install "numpy<2"
pip install nvidia-cudnn-cu11==8.7.0.84
```

### Basic Usage

```python
import asyncio
from src.services.simple_whisperx_cli import SimpleWhisperXCLI

async def transcribe_audio():
    cli = SimpleWhisperXCLI()

    result = await cli.transcribe_audio(
        audio_path="your_audio_file.wav",
        model="large-v2",           # Best quality
        enable_diarization=True,    # Speaker identification
        # GPU automatically detected and used
    )

    print(f"Success: {result['success']}")
    print(f"Device used: {result['device_used']}")
    print(f"Processing time: {result['processing_time_seconds']:.1f}s")
    print(f"Speakers detected: {result['speakers_count']}")
    print(f"Text: {result['text'][:200]}...")

# Run transcription
asyncio.run(transcribe_audio())
```

### Command Line Usage

```bash
# Activate environment
source transcribe_mcp_env/bin/activate

# Run transcription (GPU auto-detected)
python -c "
import asyncio
from src.services.simple_whisperx_cli import SimpleWhisperXCLI

async def main():
    cli = SimpleWhisperXCLI()
    result = await cli.transcribe_audio('audio.wav', model='large-v2')
    print(f'Success: {result[\"success\"]}, Device: {result[\"device_used\"]}')

asyncio.run(main())
"
```

## üìã Requirements

### Hardware Requirements

#### For GPU Acceleration (Recommended)
- **NVIDIA GPU** with CUDA Compute Capability 3.5+
- **VRAM**: 6GB+ for best models (RTX 3060/3070/3080/4070/4080/4090)
- **RAM**: 16GB+ system memory
- **Storage**: NVMe SSD recommended for large files

#### For CPU-Only Mode
- **CPU**: Modern multi-core processor
- **RAM**: 8GB+ system memory
- **Storage**: Any storage type

### Software Requirements

- **Python**: 3.8+
- **CUDA**: 11.8 (for GPU acceleration)
- **FFmpeg**: For audio format conversion
- **Linux/Windows/macOS**: Cross-platform support

## üéØ Models and Performance

### Available Models

| Model | Quality | Speed | VRAM | Use Case |
|-------|---------|-------|------|----------|
| `base` | Good | Fastest | 2GB | Development, testing |
| `small` | Better | Fast | 3GB | Balanced performance |
| `medium` | Very Good | Medium | 4GB | Production quality |
| `large` | Excellent | Slower | 6GB | High quality |
| `large-v2` | **Best** | Slower | 6GB | **Production** |

### Performance Benchmarks

Tested on RTX 3060 with large-v2 model:

| File Size | Audio Duration | Processing Time | Realtime Factor |
|-----------|----------------|-----------------|-----------------|
| 10MB | 5 min | 45s | 6.7x |
| 50MB | 25 min | 3.5min | 7.1x |
| 111MB | 47 min | 6.8min | 6.97x |

## üìä Features

### Automatic GPU Detection

The service automatically detects and configures optimal settings:

```python
# Automatic configuration
cli = SimpleWhisperXCLI()
result = await cli.transcribe_audio("audio.wav")

# Manual configuration
result = await cli.transcribe_audio(
    "audio.wav",
    device="cuda",          # Force GPU
    compute_type="float16", # GPU precision
    batch_size=16          # GPU batch size
)
```

### Speaker Diarization

Identify and label different speakers:

```python
result = await cli.transcribe_audio(
    "meeting.wav",
    enable_diarization=True
)

print(f"Speakers: {result['speakers']}")
# Output: ['SPEAKER_00', 'SPEAKER_01', 'SPEAKER_02']

for segment in result['segments'][:3]:
    speaker = segment['speaker']
    text = segment['text']
    print(f"{speaker}: {text}")
```

### Multiple Output Formats

```python
result = await cli.transcribe_audio("audio.wav")

print("Generated files:")
for format_name, file_path in result['generated_files'].items():
    print(f"  {format_name.upper()}: {file_path}")

# Output:
# JSON: /path/to/audio.json (detailed segments)
# TXT: /path/to/audio.txt (plain text)
# SRT: /path/to/audio.srt (subtitles)
# VTT: /path/to/audio.vtt (web subtitles)
# TSV: /path/to/audio.tsv (tab-separated)
```

### Performance Monitoring

```python
result = await cli.transcribe_audio("audio.wav")

# Performance metrics
print(f"Processing time: {result['processing_time_seconds']:.1f}s")
print(f"Audio duration: {result['audio_duration_seconds']:.1f}s")
print(f"Realtime factor: {result['realtime_factor']:.2f}x")
print(f"File processing speed: {result['processing_speed_mb_per_sec']:.2f} MB/s")
print(f"Device used: {result['device_used']}")
print(f"GPU available: {result['gpu_available']}")
```

## üß™ Testing

### Run Test Suite

```bash
# Activate environment
source transcribe_mcp_env/bin/activate

# Quick validation
python test_service_validation.py

# Comprehensive GPU test
python test_gpu_enhanced_service.py

# Performance benchmark (if large file available)
python large_file_gpu_test.py
```

### Validate GPU Setup

```bash
# Check CUDA availability
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"

# Check GPU details
python -c "import torch; print('GPU:', torch.cuda.get_device_name(0))"

# Test GPU tensor
python -c "import torch; x=torch.tensor([1.0]).cuda(); print('GPU working:', x.device)"
```

## üìö Documentation

### Comprehensive Guides

- **[GPU Acceleration Guide](docs/GPU_ACCELERATION_GUIDE.md)** - Complete GPU setup and optimization
- **[PyTorch/CUDA Compatibility](docs/PYTORCH_CUDA_COMPATIBILITY.md)** - Version compatibility matrix
- **[Performance Benchmarks](docs/PERFORMANCE_BENCHMARKS.md)** - Detailed performance analysis

### API Reference

```python
class SimpleWhisperXCLI:
    async def transcribe_audio(
        self,
        audio_path: str,                    # Path to audio file
        output_dir: Optional[str] = None,   # Output directory
        model: str = "base",                # Model size
        language: str = "en",               # Language code
        enable_diarization: bool = True,    # Speaker diarization
        timeout_minutes: int = 30,          # Processing timeout
        device: Optional[str] = None,       # Force device (cuda/cpu)
        compute_type: Optional[str] = None, # Precision (float16/float32)
        batch_size: Optional[int] = None    # Batch size
    ) -> Dict[str, Any]:
        """
        Transcribe audio with automatic GPU optimization.

        Returns:
            Dict containing results, performance metrics, and file paths
        """
```

## üîß Troubleshooting

### Common Issues

#### GPU Not Detected
```bash
# Install CUDA-enabled PyTorch
pip install torch==2.2.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Check GPU
python -c "import torch; print(torch.cuda.is_available())"
```

#### CUDNN Library Issues
```bash
# Install compatible CUDNN
pip install nvidia-cudnn-cu11==8.7.0.84 --force-reinstall
```

#### Out of Memory
```python
# Reduce batch size for lower VRAM
result = await cli.transcribe_audio(
    "audio.wav",
    batch_size=8,  # Reduce from default 16
    model="medium"  # Use smaller model
)
```

### Performance Optimization

#### For Production
```python
# Optimal production settings
result = await cli.transcribe_audio(
    "audio.wav",
    model="large-v2",        # Best quality
    enable_diarization=True, # Speaker ID
    device="cuda",           # Force GPU
    batch_size=16           # Optimal batch size
)
```

#### For Development
```python
# Fast development settings
result = await cli.transcribe_audio(
    "audio.wav",
    model="base",            # Fastest model
    enable_diarization=False, # Skip for speed
    timeout_minutes=5        # Short timeout
)
```

## üèóÔ∏è Architecture

### Project Structure

```
TranscribeMCP/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py                # MCP server
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fastmcp_server.py        # FastMCP implementation
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ simple_whisperx_cli.py   # Main service (GPU-enhanced)
‚îÇ   ‚îú‚îÄ‚îÄ tools/                       # MCP tool implementations
‚îÇ   ‚îî‚îÄ‚îÄ models/                      # Data models
‚îú‚îÄ‚îÄ tests/                           # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ integration/                 # Integration tests
‚îÇ   ‚îú‚îÄ‚îÄ validation/                  # Validation tests
‚îÇ   ‚îî‚îÄ‚îÄ results/                     # Test results
‚îú‚îÄ‚îÄ docs/                            # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ guides/                      # Setup and integration guides
‚îÇ   ‚îî‚îÄ‚îÄ INTEGRATION_EXAMPLES.md      # Code examples
‚îú‚îÄ‚îÄ scripts/                         # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ start_mcp_server.sh          # MCP server launcher
‚îÇ   ‚îî‚îÄ‚îÄ test_mcp_connection.py       # Connection test
‚îú‚îÄ‚îÄ test_data/                       # Sample audio files
‚îî‚îÄ‚îÄ requirements.txt                 # Dependencies
```

For detailed structure, see [docs/PROJECT_STRUCTURE.md](docs/PROJECT_STRUCTURE.md)

### Key Components

- **SimpleWhisperXCLI**: Main transcription service with GPU acceleration
- **MCP Server**: Model Context Protocol server for integrations
- **Automatic Detection**: GPU/CPU detection and optimization
- **Process Management**: Timeout protection and cleanup
- **Performance Monitoring**: Real-time metrics collection
- **Multi-format Output**: JSON, TXT, SRT, VTT, TSV generation

## üîå MCP Server Integration

TranscribeMCP provides a Model Context Protocol (MCP) server with multiple transport options:

### Server Modes

#### Stdio Mode (Claude Desktop)
```bash
# Start in stdio mode (for Claude Desktop)
transcribe-mcp stdio

# Or using legacy command
transcribe-mcp-stdio
```

#### HTTP Mode (Web Applications)
```bash
# Start HTTP server (default: SSE on port 8000)
transcribe-mcp http

# Custom host and port
transcribe-mcp http --host 127.0.0.1 --port 3000

# StreamableHTTP mode for advanced use cases
transcribe-mcp http --transport streamable_http
```

### Quick Integration

**Claude Desktop** (Stdio Mode):
```json
{
  "mcpServers": {
    "transcribe_mcp": {
      "command": "transcribe-mcp",
      "args": ["stdio"]
    }
  }
}
```

**Python Client** (Stdio Mode):
```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

server_params = StdioServerParameters(
    command="transcribe-mcp",
    args=["stdio"]
)

async with stdio_client(server_params) as (read, write):
    async with ClientSession(read, write) as session:
        await session.initialize()
        result = await session.call_tool("transcribe_audio", {...})
```

**HTTP Client** (JavaScript):
```javascript
const EventSource = require('eventsource');
const es = new EventSource('http://localhost:8000/sse');

es.onmessage = (event) => {
  console.log('Received:', event.data);
};
```

**HTTP Client** (Python):
```python
import httpx

async with httpx.AsyncClient() as client:
    async with client.stream(
        "GET",
        "http://localhost:8000/sse",
        headers={"Accept": "text/event-stream"}
    ) as response:
        async for line in response.aiter_lines():
            print(line)
```

### Documentation

- **Server Modes Guide**: [docs/SERVER_MODES.md](docs/SERVER_MODES.md) - Complete guide to stdio and HTTP modes
- **Connection Guide**: [docs/guides/MCP_CONNECTION_GUIDE.md](docs/guides/MCP_CONNECTION_GUIDE.md)
- **Quick Reference**: [docs/guides/MCP_QUICK_REFERENCE.md](docs/guides/MCP_QUICK_REFERENCE.md)
- **Integration Examples**: [docs/INTEGRATION_EXAMPLES.md](docs/INTEGRATION_EXAMPLES.md)
- **Server Status**: [docs/guides/MCP_SERVER_READY.md](docs/guides/MCP_SERVER_READY.md)

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests (`python test_service_validation.py`)
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push to branch (`git push origin feature/amazing-feature`)
6. Open Pull Request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- **OpenAI Whisper** - Foundation transcription model
- **WhisperX** - Enhanced diarization and alignment
- **PyTorch** - Machine learning framework
- **NVIDIA CUDA** - GPU acceleration platform

## üìû Support

For issues, questions, or feature requests:

1. Check the [documentation](docs/)
2. Run the validation script: `python test_service_validation.py`
3. Review [troubleshooting](#-troubleshooting) section
4. Open an issue with detailed logs and system info

---

**TranscribeMCP** - Making audio transcription fast, accurate, and accessible. üéµ‚û°Ô∏èüìù