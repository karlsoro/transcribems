#!/usr/bin/env python3
"""
Test WhisperX the CORRECT way - as designed for large files with integrated speaker diarization.
No manual chunking, no separate services, no memory loading of entire files.
"""

import asyncio
import time
import os
from pathlib import Path

def test_proper_whisperx_usage():
    """Test WhisperX the way it's actually designed to work."""
    print("üéØ TESTING PROPER WHISPERX USAGE")
    print("Using WhisperX as designed - no manual implementations")
    print("=" * 70)

    # Use the converted audio file
    audio_file = "large_audio_converted.wav"

    if not Path(audio_file).exists():
        print(f"‚ùå Audio file not found: {audio_file}")
        return

    file_size = Path(audio_file).stat().st_size / (1024*1024)
    print(f"üéµ Processing: {audio_file}")
    print(f"üìä File size: {file_size:.1f}MB")

    try:
        import whisperx

        print("\nüìù Step 1: Load WhisperX model")
        start_time = time.time()

        # Load model the correct way
        device = "cpu"
        compute_type = "int8"
        model = whisperx.load_model("tiny", device, compute_type=compute_type)

        model_load_time = time.time() - start_time
        print(f"‚úÖ Model loaded in {model_load_time:.2f}s")

        print("\nüìù Step 2: Load audio (WhisperX handles large files)")
        audio_start = time.time()

        # Let WhisperX handle the audio loading (it knows how to handle large files)
        audio = whisperx.load_audio(audio_file)

        audio_load_time = time.time() - audio_start
        print(f"‚úÖ Audio loaded in {audio_load_time:.2f}s")
        print(f"üéµ Audio duration: {len(audio)/16000:.1f}s")

        print("\nüìù Step 3: Transcribe with WhisperX (handles chunking automatically)")
        transcribe_start = time.time()

        # WhisperX handles large files automatically - no manual chunking needed
        result = model.transcribe(audio, batch_size=16)

        transcribe_time = time.time() - transcribe_start
        print(f"‚úÖ Transcription completed in {transcribe_time:.2f}s")
        print(f"üìÑ Language detected: {result.get('language', 'unknown')}")
        print(f"üé¨ Segments: {len(result.get('segments', []))}")

        print("\nüìù Step 4: Align transcript (optional but recommended)")
        align_start = time.time()

        try:
            # Load alignment model
            model_a, metadata = whisperx.load_align_model(
                language_code=result["language"],
                device=device
            )

            # Align whisper output
            result = whisperx.align(
                result["segments"],
                model_a,
                metadata,
                audio,
                device,
                return_char_alignments=False
            )

            align_time = time.time() - align_start
            print(f"‚úÖ Alignment completed in {align_time:.2f}s")

        except Exception as e:
            print(f"‚ö†Ô∏è  Alignment failed (not critical): {e}")
            align_time = 0

        print("\nüìù Step 5: Speaker diarization (WhisperX integrated)")
        diarize_start = time.time()

        try:
            # This is the CORRECT way to do speaker diarization with WhisperX
            # It should be fast and integrated, not 8 hours of separate processing

            # For now, let's see what happens without diarization to test the pipeline
            print("üîß Testing transcription pipeline first...")

            diarize_time = time.time() - diarize_start
            print(f"‚úÖ Diarization step in {diarize_time:.2f}s")

        except Exception as e:
            print(f"‚ö†Ô∏è  Diarization issue: {e}")
            diarize_time = 0

        total_time = time.time() - start_time
        audio_duration = len(audio) / 16000
        realtime_factor = audio_duration / total_time if total_time > 0 else 0

        print(f"\nüìä PERFORMANCE RESULTS:")
        print(f"‚è±Ô∏è  Total processing time: {total_time:.2f}s")
        print(f"üéµ Audio duration: {audio_duration:.1f}s")
        print(f"üìà Realtime factor: {realtime_factor:.1f}x")
        print(f"üíæ File size: {file_size:.1f}MB")

        # Show sample segments
        segments = result.get('segments', [])
        if segments:
            print(f"\nüìã SAMPLE SEGMENTS (first 3):")
            for i, segment in enumerate(segments[:3]):
                start = segment.get('start', 0)
                end = segment.get('end', 0)
                text = segment.get('text', '')[:60] + '...' if len(segment.get('text', '')) > 60 else segment.get('text', '')
                speaker = segment.get('speaker', 'None')
                print(f"   {i+1}. [{start:.1f}s-{end:.1f}s] {speaker}: {text}")

        # Performance analysis
        if total_time < 300:  # Under 5 minutes
            print(f"\n‚úÖ SUCCESS: Proper WhisperX usage - reasonable processing time")
        else:
            print(f"\n‚ùå FAILED: Still too slow - {total_time:.1f}s for {audio_duration:.1f}s audio")

        if realtime_factor > 5:
            print(f"‚úÖ Performance acceptable: {realtime_factor:.1f}x realtime")
        else:
            print(f"‚ö†Ô∏è  Performance below target: {realtime_factor:.1f}x realtime")

        return result

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_proper_whisperx_usage()