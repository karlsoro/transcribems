# TranscribeMCP MCP Server Deployment Guide

## Overview

This guide provides comprehensive instructions for deploying TranscribeMCP as a Model Context Protocol (MCP) server with GPU acceleration support.

## üöÄ **GPU-Enhanced MCP Features**

TranscribeMCP MCP Server now includes:
- **GPU Acceleration**: 7x faster processing with CUDA
- **Automatic Device Detection**: Intelligent GPU/CPU fallback
- **Performance Metrics**: Real-time processing statistics
- **Job Management**: Asynchronous processing with progress tracking
- **Multi-format Output**: JSON, SRT, VTT, TXT, TSV
- **Speaker Diarization**: Advanced speaker identification

## üìã Prerequisites

### System Requirements

#### For GPU Acceleration (Recommended)
- **NVIDIA GPU**: RTX 3060+ or equivalent (6GB+ VRAM)
- **CUDA**: Version 11.8 or compatible
- **System RAM**: 16GB+ recommended
- **Python**: 3.8+

#### For CPU-Only Deployment
- **CPU**: Modern multi-core processor
- **System RAM**: 8GB minimum, 16GB+ recommended
- **Python**: 3.8+

### Software Dependencies

```bash
# Core MCP dependencies
pip install mcp

# GPU-enhanced TranscribeMCP with dependencies
# (See installation section below)
```

## üõ†Ô∏è Installation

### 1. Clone and Setup Environment

```bash
# Clone repository
git clone <repository-url>
cd TranscribeMCP

# Create virtual environment
python -m venv transcribe_mcp_env
source transcribe_mcp_env/bin/activate  # Linux/Mac
# transcribe_mcp_env\Scripts\activate  # Windows
```

### 2. Install GPU-Enhanced Dependencies

```bash
# Install CUDA-enabled PyTorch (for GPU acceleration)
pip install torch==2.2.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install compatible NumPy
pip install "numpy<2"

# Install compatible CUDNN
pip install nvidia-cudnn-cu11==8.7.0.84

# Install project dependencies
pip install -r requirements.txt

# Install MCP SDK
pip install mcp
```

### 3. Verify GPU Setup

```bash
# Check GPU availability
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
python -c "import torch; print('GPU name:', torch.cuda.get_device_name(0))"

# Validate service
python test_service_validation.py
```

## üóÇÔ∏è Project Structure

```
TranscribeMCP/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.py                 # Main MCP server
‚îÇ   ‚îú‚îÄ‚îÄ tools/                        # MCP tools (GPU-enhanced)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcribe_tool.py        # Audio transcription
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ progress_tool.py          # Progress tracking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ result_tool.py            # Result retrieval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ history_tool.py           # Job history
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_tool.py             # Batch processing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cancel_tool.py            # Job cancellation
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_whisperx_cli.py    # GPU-enhanced service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mcp_transcription_adapter.py # MCP bridge
‚îÇ   ‚îî‚îÄ‚îÄ models/                       # Data models
‚îú‚îÄ‚îÄ docs/                             # Documentation
‚îú‚îÄ‚îÄ config/                           # Configuration files
‚îî‚îÄ‚îÄ tests/                            # Test suites
```

## üöÄ MCP Server Deployment

### 1. Start MCP Server

#### Method 1: Direct Python Execution

```bash
# Activate environment
source transcribe_mcp_env/bin/activate

# Start MCP server
python -m src.mcp_server.server
```

#### Method 2: Using MCP Command

```bash
# Install as MCP server
pip install -e .

# Start via MCP
mcp start transcribe_mcp
```

#### Method 3: Background Service

```bash
# Create systemd service (Linux)
sudo tee /etc/systemd/system/transcribe_mcp-mcp.service << EOF
[Unit]
Description=TranscribeMCP MCP Server
After=network.target

[Service]
Type=simple
User=transcribe_mcp
WorkingDirectory=/path/to/TranscribeMCP
Environment=PATH=/path/to/TranscribeMCP/transcribe_mcp_env/bin
ExecStart=/path/to/TranscribeMCP/transcribe_mcp_env/bin/python -m src.mcp_server.server
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# Enable and start service
sudo systemctl enable transcribe_mcp-mcp
sudo systemctl start transcribe_mcp-mcp
```

### 2. MCP Client Configuration

#### Claude Desktop Configuration

Add to Claude Desktop settings:

```json
{
  "mcpServers": {
    "transcribe_mcp": {
      "command": "python",
      "args": ["-m", "src.mcp_server.server"],
      "cwd": "/path/to/TranscribeMCP",
      "env": {
        "VIRTUAL_ENV": "/path/to/TranscribeMCP/transcribe_mcp_env"
      }
    }
  }
}
```

#### Custom MCP Client

```python
import asyncio
from mcp import Client

async def connect_to_transcribe_mcp():
    client = Client()
    await client.connect("stdio", command=["python", "-m", "src.mcp_server.server"])

    # List available tools
    tools = await client.list_tools()
    print("Available tools:", [tool.name for tool in tools])

    return client

# Use the client
client = asyncio.run(connect_to_transcribe_mcp())
```

## üõ†Ô∏è MCP Tools Reference

### 1. transcribe_audio

**Description**: GPU-accelerated audio transcription with speaker diarization

**Parameters**:
```json
{
  "file_path": "/path/to/audio.wav",
  "model_size": "large",
  "language": "en",
  "enable_diarization": true,
  "device": "cuda"  // Optional: auto-detected
}
```

**Response**:
```json
{
  "success": true,
  "job_id": "uuid",
  "status": "processing",
  "system_info": {
    "gpu_available": true,
    "service_type": "GPU-Enhanced SimpleWhisperXCLI"
  },
  "message": "Transcription started using GPU acceleration"
}
```

### 2. get_transcription_progress

**Description**: Get real-time progress with GPU performance metrics

**Parameters**:
```json
{
  "job_id": "uuid",  // Optional
  "all_jobs": false  // Optional: get all jobs
}
```

**Response**:
```json
{
  "success": true,
  "progress": {
    "status": "processing",
    "progress": 0.75,
    "message": "Processing with GPU acceleration..."
  }
}
```

### 3. get_transcription_result

**Description**: Retrieve completed results with performance data

**Parameters**:
```json
{
  "job_id": "uuid",
  "format": "full",  // text, segments, full, summary
  "include_metadata": true
}
```

**Response**:
```json
{
  "success": true,
  "result": {
    "text": "transcribed text...",
    "segments": [...],
    "speakers": [...],
    "metadata": {
      "device_used": "cuda",
      "gpu_available": true,
      "realtime_factor": 6.97,
      "processing_time": 45.2
    }
  }
}
```

### 4. list_transcription_history

**Description**: List recent jobs with filtering

### 5. batch_transcribe

**Description**: Process multiple files in parallel

### 6. cancel_transcription

**Description**: Cancel running jobs

## üîß Configuration

### Environment Variables

```bash
# GPU Configuration
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=8

# Logging
export TRANSCRIBE_MCP_LOG_LEVEL=INFO
export TRANSCRIBE_MCP_LOG_FILE=/var/log/transcribe_mcp.log

# Storage
export TRANSCRIBE_MCP_DATA_DIR=/var/lib/transcribe_mcp
export TRANSCRIBE_MCP_TEMP_DIR=/tmp/transcribe_mcp
```

### Configuration Files

#### config/mcp.yaml

```yaml
server:
  name: transcribe_mcp
  version: 1.0.0
  bind_address: 0.0.0.0
  port: 8080

gpu:
  auto_detect: true
  fallback_to_cpu: true
  batch_size: 16
  compute_type: float16

models:
  default: base
  available: [tiny, base, small, medium, large, large-v2]
  cache_dir: /var/cache/transcribe_mcp/models

processing:
  max_concurrent_jobs: 3
  timeout_minutes: 60
  enable_diarization: true

logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: /var/log/transcribe_mcp.log
```

## üß™ Testing MCP Deployment

### 1. Functional Testing

```bash
# Test MCP server connectivity
python -c "
import asyncio
from mcp import Client

async def test():
    client = Client()
    try:
        await client.connect('stdio', command=['python', '-m', 'src.mcp_server.server'])
        tools = await client.list_tools()
        print(f'‚úÖ MCP server running with {len(tools)} tools')
        return True
    except Exception as e:
        print(f'‚ùå MCP server failed: {e}')
        return False

asyncio.run(test())
"
```

### 2. GPU Performance Testing

```bash
# Test GPU transcription via MCP
python test_mcp_gpu_integration.py
```

### 3. Load Testing

```bash
# Test multiple concurrent requests
python test_mcp_load.py
```

## üìä Monitoring and Logging

### Log Locations

```bash
# Application logs
tail -f /var/log/transcribe_mcp.log

# System service logs (if using systemd)
journalctl -u transcribe_mcp-mcp -f

# GPU monitoring
watch -n 1 nvidia-smi
```

### Performance Metrics

The MCP server provides these metrics:

- **Processing Speed**: Realtime factor (e.g., 6.97x)
- **Device Utilization**: GPU/CPU usage
- **Job Queue**: Active and pending jobs
- **Throughput**: Files processed per hour
- **Error Rates**: Failed job percentage

### Health Checks

```bash
# Check server health
curl -X POST http://localhost:8080/health

# Check GPU status
python -c "
from src.services.mcp_transcription_adapter import MCPTranscriptionAdapter
adapter = MCPTranscriptionAdapter()
print(adapter.get_system_info())
"
```

## üîí Security Considerations

### File Access Security

```python
# Configure allowed file paths
ALLOWED_PATHS = [
    "/var/uploads/audio/",
    "/home/user/audio/",
    "/tmp/transcribe_mcp/"
]

# Validate file paths in tools
def validate_file_path(file_path):
    return any(file_path.startswith(path) for path in ALLOWED_PATHS)
```

### Authentication

```yaml
# config/security.yaml
auth:
  enabled: true
  method: token  # token, oauth, none
  tokens:
    - name: "platform-integration"
      token: "secure-random-token"
      permissions: ["transcribe", "results"]

rate_limiting:
  enabled: true
  requests_per_minute: 60
  max_file_size_mb: 500
```

## üöÄ Production Deployment

### Docker Deployment

```dockerfile
FROM nvidia/cuda:11.8-runtime-ubuntu22.04

# Install Python and dependencies
RUN apt-get update && apt-get install -y python3 python3-pip

# Copy application
COPY . /app
WORKDIR /app

# Install dependencies
RUN pip install -r requirements.txt
RUN pip install torch==2.2.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Start MCP server
CMD ["python", "-m", "src.mcp_server.server"]
```

### Docker Compose

```yaml
version: '3.8'
services:
  transcribe_mcp-mcp:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./data:/var/lib/transcribe_mcp
      - ./logs:/var/log
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TRANSCRIBE_MCP_LOG_LEVEL=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: transcribe_mcp-mcp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: transcribe_mcp-mcp
  template:
    metadata:
      labels:
        app: transcribe_mcp-mcp
    spec:
      containers:
      - name: transcribe_mcp
        image: transcribe_mcp:latest
        ports:
        - containerPort: 8080
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 16Gi
          requests:
            memory: 8Gi
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
```

## üîß Troubleshooting

### Common Issues

#### 1. GPU Not Detected

```bash
# Check CUDA installation
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Reinstall CUDA PyTorch
pip install torch==2.2.0+cu118 --force-reinstall --index-url https://download.pytorch.org/whl/cu118
```

#### 2. MCP Server Not Starting

```bash
# Check dependencies
pip install mcp
python -c "import mcp; print('MCP SDK available')"

# Check server directly
python -m src.mcp_server.server
```

#### 3. Memory Issues

```bash
# Reduce batch size for lower VRAM
export TRANSCRIBE_MCP_BATCH_SIZE=8

# Use smaller model
export TRANSCRIBE_MCP_DEFAULT_MODEL=medium
```

#### 4. Performance Issues

```bash
# Check GPU utilization
nvidia-smi

# Monitor system resources
htop

# Check logs for bottlenecks
tail -f /var/log/transcribe_mcp.log
```

## üìà Performance Optimization

### GPU Optimization

```python
# Optimal settings for different GPU types
GPU_CONFIGS = {
    "RTX 3060": {"batch_size": 16, "model": "large-v2"},
    "RTX 3070": {"batch_size": 20, "model": "large-v2"},
    "RTX 3080": {"batch_size": 24, "model": "large-v2"},
    "RTX 4090": {"batch_size": 32, "model": "large-v2"}
}
```

### Concurrent Processing

```python
# Configure for your system
MAX_CONCURRENT_JOBS = min(GPU_COUNT * 2, CPU_COUNT // 2)
```

## üîó Integration Examples

### Platform Integration

```python
# Example: Integrate with your platform
import asyncio
from mcp import Client

class TranscriptionPlatform:
    def __init__(self):
        self.mcp_client = None

    async def connect_to_transcribe_mcp(self):
        self.mcp_client = Client()
        await self.mcp_client.connect(
            "stdio",
            command=["python", "-m", "src.mcp_server.server"],
            cwd="/path/to/TranscribeMCP"
        )

    async def transcribe_file(self, file_path):
        result = await self.mcp_client.call_tool(
            "transcribe_audio",
            {"file_path": file_path, "model_size": "large"}
        )
        return result

    async def get_results(self, job_id):
        return await self.mcp_client.call_tool(
            "get_transcription_result",
            {"job_id": job_id, "format": "full"}
        )
```

## üìù API Documentation

Complete API documentation is available at:
- **MCP Tools Spec**: `specs/002-adjust-the-current/contracts/mcp_tools_spec.json`
- **Interactive Docs**: Available when server is running

## üÜò Support

For deployment issues:

1. **Check Logs**: Review application and system logs
2. **Validate Setup**: Run test scripts
3. **GPU Issues**: Check CUDA/PyTorch compatibility
4. **Performance**: Monitor system resources

## üìã Checklist

### Pre-Deployment
- [ ] GPU drivers installed and tested
- [ ] CUDA PyTorch installed and verified
- [ ] Virtual environment activated
- [ ] Dependencies installed
- [ ] Configuration files created
- [ ] Test files available

### Deployment
- [ ] MCP server starts successfully
- [ ] GPU detection working
- [ ] Tools respond correctly
- [ ] Performance acceptable
- [ ] Logs configured
- [ ] Monitoring setup

### Post-Deployment
- [ ] Health checks passing
- [ ] Performance metrics collected
- [ ] Integration tested
- [ ] Documentation updated
- [ ] Backup procedures in place

---

**TranscribeMCP MCP Server** is now production-ready with GPU acceleration, providing 7x performance improvement for audio transcription workloads. üéµ‚û°Ô∏èüìù‚ö°